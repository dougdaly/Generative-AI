{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5o5NCkcEvYkx"
   },
   "source": [
    "[![Works with Edge Impulse](https://raw.githubusercontent.com/edgeimpulse/notebooks/main/.assets/images/ei-badge.svg)](http://edgeimpulse.com) [![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/edgeimpulse/notebooks/blob/main/notebooks/generate-dall-e-image-dataset.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6Wf9MFOvYky"
   },
   "source": [
    "# Use OpenAI Dall-E to generate an image dataset for image classification\n",
    "This notebook explores how we can use generative AI to create datasets which don't exist yet. This can be a good starting point for your project if you have not collected or cannot collect the data required. It is important to note the limitations of generative AI still apply here, biases can be introduced through your prompts, results can include \"hallucinations\" and quality control is important.\n",
    "\n",
    "This example uses the openai API to call the Dall-E image generation tool, it explores both generation and variation but there are other tools such as editing which could also be useful for augmenting an existing dataset.\n",
    "\n",
    "We have wrapped this example into a [Transformation Block](https://docs.edgeimpulse.com/docs/edge-impulse-studio/organizations/research-data/creating-a-transformation-block-dataset) (Enterprise Feature) to make it even easier to generate images. Upload to your organisation here: https://github.com/edgeimpulse/example-transform-Dall-E-images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCJJFzQZvYky"
   },
   "source": [
    "\n",
    "### Local Software Requirements\n",
    "- Python 3\n",
    "- Pip package manager\n",
    "- Jupyter Notebook: https://jupyter.org/install\n",
    "- pip packages (install with `pip install `*`packagename`*):\n",
    "    - openai https://pypi.org/project/openai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0ucxRaTIvYky"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.72.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.6.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hUtxiO9_vYky"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import openai\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "# Notebook Imports\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "HOME_DIR = \"/Users/douglasdaly/Desktop/interview_kickstart\"\n",
    "sys.path.append(f\"{HOME_DIR}/support\")\n",
    "from support import get_device\n",
    "\n",
    "device = get_device()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkbLgMekvYky"
   },
   "source": [
    "## Set up OpenAI API\n",
    "First off you will need to set up and Edge Impulse account and create your first project.\n",
    "\n",
    "You will also need to create an API Key for OpenAI: https://platform.openai.com/docs/api-reference/authentication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GOuwwRPevYky"
   },
   "outputs": [],
   "source": [
    "\n",
    "# You can set your API key and org as environment variables in your system like this:\n",
    "# os.environ['OPENAI_API_KEY'] = 'api string'\n",
    "\n",
    "# Set up OpenAI API key and organization\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set env vars for the relevant model or load from a .env file:\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3Jz2A_WvYky"
   },
   "source": [
    "\n",
    "### Generate your first image\n",
    "The API takes in a prompt, number of images and a size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "MKV2m_ffvYky"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-BjiaYIjDlL6yZRrXyfRYSbPe/user-VROr7JE1i8xy8xYLEMPmf8pv/img-WJN4s6tsLyczTfGuMgkgmjL3.png?st=2025-04-13T18%3A20%3A02Z&se=2025-04-13T20%3A20%3A02Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-13T16%3A24%3A24Z&ske=2025-04-14T16%3A24%3A24Z&sks=b&skv=2024-08-04&sig=%2BiLd7KIqpDxTyL65OSIF6Ny2c/3jeOe18lGHeGYxkL8%3D\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-BjiaYIjDlL6yZRrXyfRYSbPe/user-VROr7JE1i8xy8xYLEMPmf8pv/img-WJN4s6tsLyczTfGuMgkgmjL3.png?st=2025-04-13T18%3A20%3A02Z&se=2025-04-13T20%3A20%3A02Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-13T16%3A24%3A24Z&ske=2025-04-14T16%3A24%3A24Z&sks=b&skv=2024-08-04&sig=%2BiLd7KIqpDxTyL65OSIF6Ny2c/3jeOe18lGHeGYxkL8%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_prompt = \"A webcam image of a human 1m from the camera sitting at a desk showing that they are wearing gloves with their hands up to the camera.\"\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=image_prompt,\n",
    "    size=\"1024x1024\",\n",
    "    quality=\"standard\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "print(response.data[0].url)\n",
    "display(Image(url=response.data[0].url))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0Pd9ltlvYky"
   },
   "source": [
    "### Generate some variations of this image\n",
    "The API also has a variations call which takes in an existing images and creates variations of it. This could also be used to modify existing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "_E7nlC72vYky"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-BjiaYIjDlL6yZRrXyfRYSbPe/user-VROr7JE1i8xy8xYLEMPmf8pv/img-ZMWyJ5q6DhC1IsLCpOqCbvrw.png?st=2025-04-13T18%3A20%3A54Z&se=2025-04-13T20%3A20%3A54Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-13T09%3A35%3A00Z&ske=2025-04-14T09%3A35%3A00Z&sks=b&skv=2024-08-04&sig=bEm/pgcHyvBjkKb36tzXgpO8VCbfJTELTCt4WJVHM8I%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-BjiaYIjDlL6yZRrXyfRYSbPe/user-VROr7JE1i8xy8xYLEMPmf8pv/img-OC4UnOQ1lBzOMYnkDLSJmADC.png?st=2025-04-13T18%3A20%3A53Z&se=2025-04-13T20%3A20%3A53Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-13T09%3A35%3A00Z&ske=2025-04-14T09%3A35%3A00Z&sks=b&skv=2024-08-04&sig=PSUwW4D28DzuoGc1zkV42lHOy/Js6CrCeAvaAgoJu0E%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-BjiaYIjDlL6yZRrXyfRYSbPe/user-VROr7JE1i8xy8xYLEMPmf8pv/img-LecHomywpcxjGI9tENSYbGfD.png?st=2025-04-13T18%3A20%3A54Z&se=2025-04-13T20%3A20%3A54Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-13T09%3A35%3A00Z&ske=2025-04-14T09%3A35%3A00Z&sks=b&skv=2024-08-04&sig=6vKDSuhSi/hst9puX7orUBnJbVYnBaXEo1ZaT943zyY%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response2 = client.images.create_variation(\n",
    "  image=requests.get(response.data[0].url).content,\n",
    "  n=3,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "imgs = []\n",
    "for img in response2.data:\n",
    "  imgs.append(Image(url=img.url))\n",
    "\n",
    "display(*imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfqjWl51vYkz"
   },
   "source": [
    "## Generate a dataset:\n",
    "Here we are iterate through a number of images and variations to generate a dataset based on the prompts/labels given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caJ9MWgrvYkz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 429 - {'error': {'code': 'rate_limit_exceeded', 'message': 'Rate limit exceeded for images per minute in organization org-BjiaYIjDlL6yZRrXyfRYSbPe. Limit: 5/1min. Current: 6/1min. Please visit https://platform.openai.com/docs/guides/rate-limits to learn how to increase your rate limit.', 'param': None, 'type': 'requests'}}\n",
      "Error code: 429 - {'error': {'code': 'rate_limit_exceeded', 'message': 'Rate limit exceeded for images per minute in organization org-BjiaYIjDlL6yZRrXyfRYSbPe. Limit: 5/1min. Current: 6/1min. Please visit https://platform.openai.com/docs/guides/rate-limits to learn how to increase your rate limit.', 'param': None, 'type': 'requests'}}\n",
      "Error code: 429 - {'error': {'code': 'rate_limit_exceeded', 'message': 'Rate limit exceeded for images per minute in organization org-BjiaYIjDlL6yZRrXyfRYSbPe. Limit: 5/1min. Current: 6/1min. Please visit https://platform.openai.com/docs/guides/rate-limits to learn how to increase your rate limit.', 'param': None, 'type': 'requests'}}\n"
     ]
    }
   ],
   "source": [
    "labels = [{\"prompt\": \"A webcam image of a human 1m from the camera sitting at a desk showing that they are wearing wool gloves with their hands up to the camera.\",\n",
    "          \"label\": \"gloves\"},\n",
    "          {\"prompt\": \"A webcam image of a person 1m from the camera sitting at a desk with their bare hands up to the camera.\",\n",
    "          \"label\": \"no-gloves\"}\n",
    "        ]\n",
    "output_folder = \"output\"\n",
    "base_images_number = 10\n",
    "variation_per_image = 3\n",
    "# Check if output directory for noisey files exists and create it if it doesn't\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for option in labels:\n",
    "    for i in range(base_images_number):\n",
    "        response = client.images.generate(\n",
    "            model=\"dall-e-3\",\n",
    "            prompt=option[\"prompt\"],\n",
    "            size=\"1024x1024\",\n",
    "            quality=\"standard\",\n",
    "            n=1,\n",
    "        )\n",
    "        try:\n",
    "            img = response.data[0].url\n",
    "            with open(f'{output_folder}/{option[\"label\"]}.{img.split(\"/\")[-1]}.png', 'wb+') as f:\n",
    "                f.write(requests.get(img).content)\n",
    "            response2 = client.images.create_variation(\n",
    "              image=requests.get(img).content,\n",
    "              n=variation_per_image,\n",
    "              size=\"1024x1024\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        for img in response2.data:\n",
    "            try:\n",
    "                with open(f'{output_folder}/{option[\"label\"]}.{img.url.split(\"/\")[-1]}.png', 'wb') as f:\n",
    "                    f.write(requests.get(img.url).content)\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYPGQoXYvYkz"
   },
   "source": [
    "### Plot all the output images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Q9zvx28vYkz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Define the folder containing the images\n",
    "folder_path = './output'\n",
    "\n",
    "# Get a list of all the image files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.png')]\n",
    "\n",
    "# Set up the plot\n",
    "fig, axs = plt.subplots(nrows=20, ncols=20, figsize=(10, 10))\n",
    "\n",
    "# Loop through each image and plot it in a grid cell\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        img = mpimg.imread(os.path.join(folder_path, image_files[i*10+j]))\n",
    "        axs[i,j].imshow(img)\n",
    "        axs[i,j].axis('off')\n",
    "\n",
    "# Make the plot look clean\n",
    "fig.subplots_adjust(hspace=0, wspace=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_jo55XDvYkz"
   },
   "source": [
    "These files can then be uploaded to a project with these commands (run in a separate terminal window):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jT_tjpT5vYkz"
   },
   "outputs": [],
   "source": [
    "! cd output\n",
    "! edge-impulse-uploader ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JZKXfimvYkz"
   },
   "source": [
    "(run edge-impulse-uploader --clean if you have used the CLI before to reset the target project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIilKXqvvYkz"
   },
   "source": [
    "## What next?\n",
    "Now you can use your images to create an image classification model on Edge Impulse.\n",
    "\n",
    "Why not try some other OpenAI calls, 'edit' could be used to take an existing image and translate it into different environments or add different humans to increase the variety of your dataset. https://platform.openai.com/docs/guides/images/usage"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "bad1f34850cf7a418228d1fe01ba985e30c9d9c31c0cf028909436fe9d6b20cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
